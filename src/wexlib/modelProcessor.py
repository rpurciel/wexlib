import os
import json
from datetime import datetime
import uuid
from functools import partial

from multiprocess import Pool, RLock, freeze_support
import numpy as np
import pandas as pd
import xarray as xr
import cfgrib
import metpy
from metpy.units import units
import metpy.calc as mpcalc
# from multiprocess import Pool, RLock, freeze_support
from tqdm.auto import tqdm
from tqdm.contrib.concurrent import process_map, thread_map

from processorGeneric import _ProcessorGeneric

def _single_sounding_data(working_dir: dict,
                          selected_chunk: dict,
                          sounding_lat: float, 
                          sounding_lon: float, 
                          time: pd.Timestamp,
                          sounding_name: str,
                          selected_file: dict,
                          **kwargs):
        """
        Generates a CSV for plotting in RAOB from input HRRR file.

        Inputs: Path to HRRR data file, directory to save output to,
                latitude and longitude for sounding. 

        CSV is saved to output directory with autogenerated name,
        based on input file. Only able to process a single time
        (the first time) from a single file.

        Returns: Success code, time (in seconds) for function to run,
                 path to output file
        """

        time_obj = pd.to_datetime(time)
        file_time_obj = pd.to_datetime(selected_file['time'])

        if sounding_lon < 0:
            sounding_lon += 360

        if selected_file['data_format'] == 'grib':
            engine = 'cfgrib'
        else:
            engine = 'netcdf4'

        if selected_chunk['model_name'] == "hrrr": #HRRR

            data = xr.open_dataset(selected_file['file_path'], 
                                 engine = engine,
                                 filter_by_keys = {'typeOfLevel': 'isobaricInhPa'},
                                 errors='ignore',)

            abslat = np.abs(data.latitude-sounding_lat)
            abslon = np.abs(data.longitude-sounding_lon)
            c = np.maximum(abslon, abslat)
            ([idx_y], [idx_x]) = np.where(c == np.min(c))

            data_pt = data.sel(y=idx_y, x=idx_x)
            selected_point = (data_pt.latitude.data, data_pt.longitude.data)

            p_mb = data_pt.isobaricInhPa.data * units("hPa")
            T_K = data_pt.t.data * units("K")
            T_C = T_K.to(units("degC"))
            Td_K = data_pt.dpt.data * units("K")
            Td_C = Td_K.to(units("degC"))
            u_ms = data_pt.u.data * units("m/s")
            u_kt = u_ms.to(units("knot"))
            v_ms = data_pt.v.data * units("m/s")
            v_kt = v_ms.to(units("knot"))
            z_gpm = data_pt.gh.data * units("gpm")

            sfc_data = xr.open_dataset(selected_file['file_path'], 
                         engine = engine,
                         filter_by_keys = {'typeOfLevel': 'surface', 'stepType': 'instant'},
                         errors='ignore',)
            sfc_data_pt = sfc_data.sel(y=idx_y, x=idx_x)

            psfc_Pa = sfc_data_pt.sp.data * units("Pa")
            psfc_mb = psfc_Pa.to(units("hPa"))
            groundlvl_m = sfc_data_pt.orog.data * units("m")

            data_2m = xr.open_dataset(selected_file['file_path'], 
                        engine = engine,
                        filter_by_keys = {'typeOfLevel': 'heightAboveGround', 'level' : 2},
                        errors='ignore',)
            data_2m_pt = data_2m.sel(y=idx_y, x=idx_x)

            T2m_K = data_2m_pt.t2m.data * units("K")
            T2m_C = T2m_K.to(units("degC"))
            Td2m_K = data_2m_pt.d2m.data * units("K")
            Td2m_C = Td2m_K.to(units("degC"))

            p2m_mb = psfc_mb - (0.2 * units("hPa"))
            lvl2m_m = groundlvl_m + (2.0 * units("m"))

            data_10m = xr.open_dataset(selected_file['file_path'], 
                     engine = engine,
                     filter_by_keys = {'typeOfLevel': 'heightAboveGround', 'level': 10},
                     errors='ignore',)
            data_10m_pt = data_10m.sel(y=idx_y, x=idx_x)

            u10m_ms = data_10m_pt.u10.data * units("m/s")
            u10m_kt = u10m_ms.to(units("knot"))
            v10m_ms = data_10m_pt.v10.data * units("m/s")
            v10m_kt = v10m_ms.to(units("knot"))

            ua_df = pd.DataFrame(data=[p_mb.magnitude, T_C.magnitude, Td_C.magnitude, u_kt.magnitude, v_kt.magnitude, z_gpm.magnitude])
            sfc_df = pd.DataFrame(data=[p2m_mb.magnitude, T2m_C.magnitude, Td2m_C.magnitude, u10m_kt.magnitude, v10m_kt.magnitude, lvl2m_m.magnitude])
            sounding_df = pd.concat([sfc_df.T, ua_df.T],
                                    axis=0,
                                    ignore_index=True)

        elif selected_chunk['model_name'] == "gfs":
            data = xr.open_dataset(selected_file['file_path'], 
                     engine = engine,
                     filter_by_keys = {'typeOfLevel': 'isobaricInhPa'},
                     errors='ignore',)

            data_pt = data.sel(latitude=sounding_lat, longitude=sounding_lon, method='nearest')
            selected_point = (data_pt.latitude.data, data_pt.longitude.data)

            p_mb = data_pt.isobaricInhPa.data * units("hPa")
            T_K = data_pt.t.data * units("K")
            T_C = T_K.to(units("degC"))
            rh = data_pt.r.data * units("%")
            Td_C = mpcalc.dewpoint_from_relative_humidity(T_C, rh)
            u_ms = data_pt.u.data * units("m/s")
            u_kt = u_ms.to(units("knot"))
            v_ms = data_pt.v.data * units("m/s")
            v_kt = v_ms.to(units("knot"))
            z_gpm = data_pt.gh.data * units("gpm")

            sfc_data = xr.open_dataset(selected_file['file_path'], 
                         engine = engine,
                         filter_by_keys = {'typeOfLevel': 'surface', 'stepType': 'instant'},
                         errors='ignore')
            sfc_data_pt = sfc_data.sel(latitude=sounding_lat, longitude=sounding_lon, method='nearest')

            psfc_Pa = sfc_data_pt.sp.data * units("Pa")
            psfc_mb = psfc_Pa.to(units("hPa"))
            groundlvl_m = sfc_data_pt.orog.data * units("m")

            data_2m = xr.open_dataset(selected_file['file_path'], 
                        engine = engine,
                        filter_by_keys = {'typeOfLevel': 'heightAboveGround', 'level' : 2},
                        errors='ignore')
            data_2m_pt = data_2m.sel(latitude=sounding_lat, longitude=sounding_lon, method='nearest')

            T2m_K = data_2m_pt.t2m.data * units("K")
            T2m_C = T2m_K.to(units("degC"))
            Td2m_K = data_2m_pt.d2m.data * units("K")
            Td2m_C = Td2m_K.to(units("degC"))
            p2m_mb = psfc_mb - (0.2 * units("hPa"))
            lvl2m_m = groundlvl_m + (2.0 * units("m"))

            data_10m = xr.open_dataset(selected_file['file_path'], 
                     engine = engine,
                     filter_by_keys = {'typeOfLevel': 'heightAboveGround', 'level': 10},
                     errors='ignore')
            data_10m_pt = data_10m.sel(latitude=sounding_lat, longitude=sounding_lon, method='nearest')

            u10m_ms = data_10m_pt.u10.data * units("m/s")
            u10m_kt = u10m_ms.to(units("knot"))
            v10m_ms = data_10m_pt.v10.data * units("m/s")
            v10m_kt = v10m_ms.to(units("knot"))

            ua_df = pd.DataFrame(data=[p_mb.magnitude, T_C.magnitude, Td_C.magnitude, u_kt.magnitude, v_kt.magnitude, z_gpm.magnitude])
            sfc_df = pd.DataFrame(data=[p2m_mb.magnitude, T2m_C.magnitude, Td2m_C.magnitude, u10m_kt.magnitude, v10m_kt.magnitude, lvl2m_m.magnitude])
            sounding_df = pd.concat([sfc_df.T, ua_df.T],
                                    axis=0,
                                    ignore_index=True)

        #Remove NaN's for winds
        sounding_df[3].fillna("", inplace=True)
        sounding_df[4].fillna("", inplace=True)

        #Filter out levels below the surface
        sounding_df = sounding_df[sounding_df[0] <= psfc_mb.magnitude]
        sounding_df = sounding_df.round(decimals=2)

        if not sounding_name:
            raob_id = selected_file['file_name']
            file_name = f"RAOBsounding_pt{time_obj.strftime('%Y%m%d_%H%M%SZ')}_{selected_chunk['model_name']}{file_time_obj.strftime('%Y%m%d_%H%M%SZ')}.csv"
        else:
            raob_id = sounding_name
            file_name = f"{sounding_name}_RAOBsounding_pt{time_obj.strftime('%Y%m%d_%H%M%SZ')}_{selected_chunk['model_name']}{file_time_obj.strftime('%Y%m%d_%H%M%SZ')}.csv"

        raob_header = {0:['RAOB/CSV','DTG','LAT','LON','ELEV','MOISTURE','WIND','GPM','MISSING','RAOB/DATA','PRES'],
                       1:[raob_id,selected_file['time'],selected_point[0],selected_point[1],round(lvl2m_m.magnitude),'TD','kts','MSL',-999,'','TEMP'],
                       2:['','','N','W','m','','U/V','','','','TD'],
                       3:['','','','','','','','','','','UU'],
                       4:['','','','','','','','','','','VV'],
                       5:['','','','','','','','','','','GPM']}

        header_df = pd.DataFrame(data=raob_header)
        raob_df = pd.concat([header_df,sounding_df], axis=0, ignore_index=True)

        output_dir = os.path.join(working_dir, 'RAOB Soundings', selected_chunk['model_name'])
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)

        output_path = os.path.join(output_dir, file_name)
        raob_df.to_csv(output_path, index=False, header=False)


class ModelProcessor(_ProcessorGeneric):

    def archive_download(self, 
                         start_time: datetime, 
                         end_time: datetime, 
                         product=None,
                         *args,
                         **kwargs):
        '''
        Download data from an archive.

        Default mode of download is to download analysis model
        files with a period equal to the model run period.
        (e.g. 24 hours of GFS would return 4 analysis files,
        for 0, 6, 12, and 18z). Different modes can be specified via flags.
        '''

        selected_version = "NA"

        if 'download_as_forecast' in kwargs:
            dl_fcst = kwargs.get('download_as_forecast')

        #user selected period is coming later LOL

        # if 'period' in kwargs:
        #     def_period = kwargs.get('period')

        #     if pandas.PeriodDtype(freq=def_period).freq != pandas.PeriodDtype(freq=self.run_period).freq:
        #         if dl_fcst:
        #             #Case FXX: Forecast data is downloaded, up to length of forecast.
        #             period = pandas.period_range(start=start_time, end=end_time, freq=def_period)
        #         else:
        #             #Case ANL(x): analysis data is preferred, filled with forecast data when analysis data is
        #             #not available. Filling only happens if input period < run period.
        #             period = pandas.period_range(start=start_time, end=end_time, freq=def_period)
        #             fill_with_fcst = True
        #             dl_fcst = False
        # else:
        #     #CASE FXX: Forecast data is downloaded, up to length of forecast.
        #     #CASE ANL(b): Only analysis data is downloaded.
        #     #difference is decided at runtime based on dl_fcst flag above.
        #     period = pandas.period_range(start=start_time, end=end_time, freq=self.run_period)

        period = pd.period_range(start=start_time, end=end_time, freq=freq)

        if self.storage_type == "aws":
            self._archive_download_aws(period, self.run_period)
        elif self.storage_type == "cds":
            self._archive_download_cds(period, self.run_period)

        print("All files finished downloading.")

    def _archive_download_aws(self, period_range, freq, product):

        chunk_uuid = str(uuid.uuid4())
        chunk_entry = {
            "chunk_uuid": chunk_uuid,
            "id": self.product_id,
            "model_name": self.internal_name,
            "product": selected_version,
            "period_start": start_time.strftime("%Y-%m-%d %H:%M:%S"),
            "period_end": end_time.strftime("%Y-%m-%d %H:%M:%S"),
            "download_frequency": freq,
            "files": [],
            "errors": []
        }

        keys = []
        file_names = []

        for time in period_range:
            file_uuid = str(uuid.uuid4())

            year = str(time.year).zfill(4)
            month = str(time.month).zfill(2)
            day = str(time.day).zfill(2)
            hour = str(time.hour).zfill(2)
            minute = str(time.minute).zfill(2)
            product = "conus"
            fcst_hr = "00" #f000 = gfs, 00 = hrrr

            tags = {}

            # floored_to_anl = time.floor(freq=self.run_period)
            # start_floored_to_anl = start_time.floor(freq=self.run_period)
            # time_offset_from_start = round((time - start_time).total_seconds())
            # time_offset_from_start_anl = round((time - start_floored_to_anl).total_seconds())
            # time_offset_from_anl_before_start = round((time - start_time.floor(freq=self.run_period)).total_seconds())
            # on_anl_time = lambda x: True if x % self.run_period_sec == 0 else False

            # if on_anl_time(time_offset):
            #     if fill_with_fcst and not dl_fcst:
            #         #CASE ANL(x) on analysis time
            #         year = str(time.year).zfill(4)
            #         month = str(time.month).zfill(2)
            #         day = str(time.day).zfill(2)
            #         hour = str(time.hour).zfill(2)
            #         fcst_hr = "anl"
            #     elif dl_fcst and not fill_with_fcst:
            #         #CASE FXX
            #         year = str(start_time.year).zfill(4)
            #         month = str(start_time.month).zfill(2)
            #         day = str(start_time.day).zfill(2)
            #         hour = str(start_time.hour).zfill(2)
            #         fcst_hr = "anl"


            # else:
            #     #This will never be entered for CASE ANL(b)
            #     if fill_with_fcst:
            #         #CASE ANL(x) off analysis time
            #         year = str(floored_to_anl.year).zfill(4)
            #         month = str(floored_to_anl.month).zfill(2)
            #         day = str(floored_to_anl.day).zfill(2)
            #         hour = str(floored_to_anl.hour).zfill(2)
            #         hrs_from_start = time_offset_from_start / 3600
            #         fcst_hr = f"f{hrs_from_start.zfill(3)}"
            #     elif dl_fcst and not fill_with_fcst:
            #         #CASE FXX 
            #         year = str(start_time.year).zfill(4)
            #         month = str(start_time.month).zfill(2)
            #         day = str(start_time.day).zfill(2)
            #         hour = str(start_time.hour).zfill(2)
            #         hrs_from_start = time_offset_from_start / 3600
            #         fcst_hr = f"f{hrs_from_start.zfill(3)}"
                


            #TODO: Add in code to check if period is on a runtime interval,
            #for discernment between 

            key_pattern = self.aws_key_patterns[selected_version].format(**locals())
            if self._aws_file_exists(self.aws_bucket_id, key_pattern):
                keys += [key_pattern]
                file_name = f"{self.internal_name}{product}.{year}{month}{day}{hour}.{fcst_hr}.{chunk_uuid}.{file_uuid}{self.file_ext}"
                file_names += [file_name]

                file_entry = {
                    "uuid": file_uuid,
                    "file_name": file_name,
                    "file_path": os.path.join(self.storage_dir, file_name),
                    "data_format": self.data_format,
                    "time": time.strftime("%Y-%m-%d %H:%M:%S"),
                    "year": year,
                    "month": month,
                    "day": day,
                    "hour": hour,
                    "minute": minute,
                    "forecast_hour": fcst_hr,
                    "tags": tags
                }

                files_list = chunk_entry['files']
                if not files_list:
                    files_list = [file_entry,]
                else:
                    files_list += [file_entry,]
                chunk_entry.update({'files': files_list})

            else:

                error_entry = {
                    "what": "download.AWSNotFound",
                    "info": f"Bucket={self.aws_bucket_id}, Key={key_pattern}, Time={time.strftime('%Y-%m-%d %H:%M:%S')}"
                }
                errors_list = chunk_entry['errors']
                if not errors_list:
                    errors_list = [error_entry,]
                else:
                    errors_list += [error_entry,]
                chunk_entry.update({'errors': errors_list})

        self._create_boto3_session()

        self._aws_download_multithread(self.storage_dir, 
                                       self.aws_bucket_id, 
                                       keys, file_names)

        self.selected_chunk = chunk_entry
        self._add_catalog_chunk('model', chunk_entry)

    def _archive_download_cds(self, period_range, freq):

        chunk_uuid = str(uuid.uuid4())
        chunk_entry = {
            "chunk_uuid": chunk_uuid,
            "id": self.product_id,
            "model_name": self.internal_name,
            "product": "N/A",
            "period_start": start_time.strftime("%Y-%m-%d %H:%M:%S"),
            "period_end": end_time.strftime("%Y-%m-%d %H:%M:%S"),
            "download_frequency": freq,
            "files": [],
            "errors": []
        }

        requests = []
        file_names = []

        era5_product = 

        for time in period_range:
            file_uuid = str(uuid.uuid4())

            year = str(time.year).zfill(4)
            month = str(time.month).zfill(2)
            day = str(time.day).zfill(2)
            hour = str(time.hour).zfill(2)
            minute = str(time.minute).zfill(2)
            fcst_hr = "NA"

            request = {
                "product_type": "reanalysis",
                "variable": "temperature",
                "pressure_level": "500",
                "format": "grib",
                "year": year,
                "month": month,
                "day": day,
                "time": f"{hour}:{minute}"
            }

            tags = {}
            requests += [request]
            file_name = f"{self.internal_name}{product}.{year}{month}{day}{hour}.{fcst_hr}.{chunk_uuid}.{file_uuid}{self.file_ext}"
            file_names += [file_name]

            file_entry = {
                "uuid": file_uuid,
                "file_name": file_name,
                "file_path": os.path.join(self.storage_dir, file_name),
                "data_format": self.data_format,
                "time": time.strftime("%Y-%m-%d %H:%M:%S"),
                "year": year,
                "month": month,
                "day": day,
                "hour": hour,
                "minute": minute,
                "forecast_hour": "fcst_hr",
                "tags": tags
            }

            files_list = chunk_entry['files']
            if not files_list:
                files_list = [file_entry,]
            else:
                files_list += [file_entry,]
            chunk_entry.update({'files': files_list})

            else:

                error_entry = {
                    "what": "download.AWSNotFound",
                    "info": f"Bucket={self.aws_bucket_id}, Key={key_pattern}, Time={time.strftime('%Y-%m-%d %H:%M:%S')}"
                }
                errors_list = chunk_entry['errors']
                if not errors_list:
                    errors_list = [error_entry,]
                else:
                    errors_list += [error_entry,]
                chunk_entry.update({'errors': errors_list})

        self._create_boto3_session()

        self._aws_download_multithread(self.storage_dir, 
                                       self.aws_bucket_id, 
                                       keys, file_names)

        self.selected_chunk = chunk_entry
        self._add_catalog_chunk('model', chunk_entry)


    def svar(self, vars):
        pass

    def sounding(self, product, *args, **kwargs):
        '''
        sounding('cross-section', Path)
        sounding('time-height', start_time, end_time, lat, lon)
        sounding('single', time, lat, lon)

        Keyword Arguments for Cross-Section:

        force_time: datetime.datetime (optional)
            Forces the selector to use a defined time for creating
            all of the soundings. Raises an exception if the defined
            time does not match with a data file.
        '''

        if self.selected_chunk == None:
            raise ValueError("No data chunk has been loaded into memory. Please assign one using 'load_chunk(chunk_uuid)'.")

        if product == "cross-section":
            self._cross_section_sounding_worker(args[0], **kwargs)

        elif product == "time-height":
            start_time = args[0]
            end_time = args[1]
            lat = args[2]
            lon = args[3]
        elif product == "single":
            raise NotImplementedError
        else:
            raise NotImplementedError

    def _cross_section_sounding_worker(self, Path, *args, **kwargs):
        input_points = []
        self.used_points = []

        force_time = kwargs.get('force_time')
        if force_time:
            try:
                pd.to_datetime(force_time)
            except Exception:
                raise Exception

        for lat, lon, time, name in zip(Path.lats, Path.lons, Path.times, Path.names):

            if force_time:
                file_time = force_time
            else:
                file_time = time

            use_point, selected_file = self._select_file_and_filter(lat, lon, file_time)

            if use_point:
                this_point = (lat, lon, time, name, selected_file)
                input_points += [this_point]

        for point in input_points:
            print(point)

        tqdm.set_lock(RLock())
        p = Pool(initializer=tqdm.set_lock, initargs=(tqdm.get_lock(),))
        p.starmap(partial(_single_sounding_data, self.working_dir, self.selected_chunk), input_points)


    def _select_file_and_filter(self,
                                sounding_lat: float, 
                                sounding_lon: float, 
                                time: pd.Timestamp,
                                skip_duplicate_points=True,
                                **kwargs):
        """
        Generates a CSV for plotting in RAOB from input HRRR file.

        Inputs: Path to HRRR data file, directory to save output to,
                latitude and longitude for sounding. 

        CSV is saved to output directory with autogenerated name,
        based on input file. Only able to process a single time
        (the first time) from a single file.

        Returns: Success code, time (in seconds) for function to run,
                 path to output file
        """

        data_period_start = datetime.strptime(self.selected_chunk['period_start'], "%Y-%m-%d %H:%M:%S")
        data_period_end = datetime.strptime(self.selected_chunk['period_end'], "%Y-%m-%d %H:%M:%S")
        data_period_freq = self.selected_chunk['download_frequency']

        time_obj = pd.to_datetime(time)
        nearest_time = time_obj.round(freq=data_period_freq)

        if nearest_time > data_period_end:
            nearest_time = data_period_end

        if nearest_time < data_period_start:
            nearest_time = data_period_start

        ### !!!!!!!!! WARNING
        ### This is not working for HRRR and is giving errors.
        ### !!!!!!!!! MUST CHANGE FOR GFS/ERA5    

        # if self.sounding_params['requires_lon_conversion'] == True and sounding_lon < 0:
        #     sounding_lon += 360

        if sounding_lon < 0:
            sounding_lon += 360

        data_files = self.selected_chunk['files']

        selected_file = {}
        for file in data_files:
            if pd.to_datetime(file['time']) == nearest_time:
                selected_file = file


        if not selected_file:
            raise ValueError("Data file for this time not found.")

        if selected_file['data_format'] == 'grib':
            engine = 'cfgrib'
        else:
            engine = 'netcdf4'

        if self.selected_chunk['model_name'] == "hrrr": #HRRR

            data = xr.open_dataset(selected_file['file_path'], 
                                 engine = engine,
                                 filter_by_keys = {'typeOfLevel': 'isobaricInhPa', 'shortName' : 't'},
                                 errors='ignore',)

            abslat = np.abs(data.latitude-sounding_lat)
            abslon = np.abs(data.longitude-sounding_lon)
            c = np.maximum(abslon, abslat)
            ([idx_y], [idx_x]) = np.where(c == np.min(c))
            data_pt = data.sel(y=idx_y, x=idx_x)

            print('Requested pt:', sounding_lat, sounding_lon)
            print('  Nearest pt:', data_pt.latitude.data, data_pt.longitude.data)

            # data_pt = data.sel(lon=sounding_lon, lat=sounding_lat, method='nearest')

        elif self.selected_chunk['model_name'] == "gfs":

            data = xr.open_dataset(selected_file['file_path'], 
                     engine = engine,
                     filter_by_keys = {'typeOfLevel': 'isobaricInhPa', 'shortName' : 't'},
                     errors='ignore')

            data_pt = data.sel(longitude=sounding_lon, latitude=sounding_lat, method='nearest')

        # data.close()

        selected_point = (data_pt.latitude.data, data_pt.longitude.data)

        #print(f"({sounding_lat}, {sounding_lon}), ", selected_point)
        if skip_duplicate_points:
            points_to_skip = self.used_points.copy()
            if selected_point in points_to_skip:
                return False, selected_file
            else:
                points_to_skip += [selected_point]
                self.used_points = points_to_skip
                return True, selected_file


    # def _single_sounding_data(self,
    #                           sounding_lat: float, 
    #                           sounding_lon: float, 
    #                           time: pd.Timestamp,
    #                           sounding_name: str,
    #                           selected_file: dict,
    #                           **kwargs):
    #     """
    #     Generates a CSV for plotting in RAOB from input HRRR file.

    #     Inputs: Path to HRRR data file, directory to save output to,
    #             latitude and longitude for sounding. 

    #     CSV is saved to output directory with autogenerated name,
    #     based on input file. Only able to process a single time
    #     (the first time) from a single file.

    #     Returns: Success code, time (in seconds) for function to run,
    #              path to output file
    #     """

    #     time_obj = pd.to_datetime(time)
    #     file_time_obj = pd.to_datetime(selected_file['time'])

    #     if self.sounding_params['requires_lon_conversion'] == True and sounding_lon < 0:
    #         sounding_lon += 360

    #     if selected_file['data_format'] == 'grib':
    #         engine = 'cfgrib'
    #     else:
    #         engine = 'netcdf4'

    #     if self.selected_chunk['model_name'] == "hrrr": #HRRR

    #         data = xr.open_dataset(selected_file['file_path'], 
    #                              engine = engine,
    #                              filter_by_keys = {'typeOfLevel': 'isobaricInhPa'},
    #                              errors='ignore',)

    #         abslat = np.abs(data.latitude-sounding_lat)
    #         abslon = np.abs(data.longitude-sounding_lon)
    #         c = np.maximum(abslon, abslat)
    #         ([idx_y], [idx_x]) = np.where(c == np.min(c))

    #         data_pt = data.sel(y=idx_y, x=idx_x)
    #         selected_point = (data_pt.latitude.data, data_pt.longitude.data)

    #         p_mb = data_pt.isobaricInhPa.data * units("hPa")
    #         T_K = data_pt.t.data * units("K")
    #         T_C = T_K.to(units("degC"))
    #         Td_K = data_pt.dpt.data * units("K")
    #         Td_C = Td_K.to(units("degC"))
    #         u_ms = data_pt.u.data * units("m/s")
    #         u_kt = u_ms.to(units("knot"))
    #         v_ms = data_pt.v.data * units("m/s")
    #         v_kt = v_ms.to(units("knot"))
    #         z_gpm = data_pt.gh.data * units("gpm")

    #         sfc_data = xr.open_dataset(selected_file['file_path'], 
    #                      engine = engine,
    #                      filter_by_keys = {'typeOfLevel': 'surface', 'stepType': 'instant'},
    #                      errors='ignore',)
    #         sfc_data_pt = sfc_data.sel(y=idx_y, x=idx_x)

    #         psfc_Pa = sfc_data_pt.sp.data * units("Pa")
    #         psfc_mb = psfc_Pa.to(units("hPa"))
    #         groundlvl_m = sfc_data_pt.orog.data * units("m")

    #         data_2m = xr.open_dataset(selected_file['file_path'], 
    #                     engine = engine,
    #                     filter_by_keys = {'typeOfLevel': 'heightAboveGround', 'level' : 2},
    #                     errors='ignore',)
    #         data_2m_pt = data_2m.sel(y=idx_y, x=idx_x)

    #         T2m_K = data_2m_pt.t2m.data * units("K")
    #         T2m_C = T2m_K.to(units("degC"))
    #         Td2m_K = data_2m_pt.d2m.data * units("K")
    #         Td2m_C = Td2m_K.to(units("degC"))

    #         p2m_mb = psfc_mb - (0.2 * units("hPa"))
    #         lvl2m_m = groundlvl_m + (2.0 * units("m"))

    #         data_10m = xr.open_dataset(selected_file['file_path'], 
    #                  engine = engine,
    #                  filter_by_keys = {'typeOfLevel': 'heightAboveGround', 'level': 10},
    #                  errors='ignore',)
    #         data_10m_pt = data_10m.sel(y=idx_y, x=idx_x)

    #         u10m_ms = data_10m_pt.u10.data * units("m/s")
    #         u10m_kt = u10m_ms.to(units("knot"))
    #         v10m_ms = data_10m_pt.v10.data * units("m/s")
    #         v10m_kt = v10m_ms.to(units("knot"))

    #         ua_df = pd.DataFrame(data=[p_mb.magnitude, T_C.magnitude, Td_C.magnitude, u_kt.magnitude, v_kt.magnitude, z_gpm.magnitude])
    #         sfc_df = pd.DataFrame(data=[p2m_mb.magnitude, T2m_C.magnitude, Td2m_C.magnitude, u10m_kt.magnitude, v10m_kt.magnitude, lvl2m_m.magnitude])
    #         sounding_df = pd.concat([sfc_df.T, ua_df.T],
    #                                 axis=0,
    #                                 ignore_index=True)

    #     elif self.selected_chunk['model_name'] == "gfs":
    #         data = xr.open_dataset(selected_file['file_path'], 
    #                  engine = engine,
    #                  filter_by_keys = {'typeOfLevel': 'isobaricInhPa'},
    #                  errors='ignore',)

    #         data_pt = data.sel(latitude=sounding_lat, longitude=sounding_lon, method='nearest')
    #         selected_point = (data_pt.latitude.data, data_pt.longitude.data)

    #         p_mb = data_pt.isobaricInhPa.data * units("hPa")
    #         T_K = data_pt.t.data * units("K")
    #         T_C = T_K.to(units("degC"))
    #         rh = data_pt.r.data * units("%")
    #         Td_C = mpcalc.dewpoint_from_relative_humidity(T_C, rh)
    #         u_ms = data_pt.u.data * units("m/s")
    #         u_kt = u_ms.to(units("knot"))
    #         v_ms = data_pt.v.data * units("m/s")
    #         v_kt = v_ms.to(units("knot"))
    #         z_gpm = data_pt.gh.data * units("gpm")

    #         sfc_data = xr.open_dataset(selected_file['file_path'], 
    #                      engine = engine,
    #                      filter_by_keys = {'typeOfLevel': 'surface', 'stepType': 'instant'},
    #                      errors='ignore')
    #         sfc_data_pt = sfc_data.sel(latitude=sounding_lat, longitude=sounding_lon, method='nearest')

    #         psfc_Pa = sfc_data_pt.sp.data * units("Pa")
    #         psfc_mb = psfc_Pa.to(units("hPa"))
    #         groundlvl_m = sfc_data_pt.orog.data * units("m")

    #         data_2m = xr.open_dataset(selected_file['file_path'], 
    #                     engine = engine,
    #                     filter_by_keys = {'typeOfLevel': 'heightAboveGround', 'level' : 2},
    #                     errors='ignore')
    #         data_2m_pt = data_2m.sel(latitude=sounding_lat, longitude=sounding_lon, method='nearest')

    #         T2m_K = data_2m_pt.t2m.data * units("K")
    #         T2m_C = T2m_K.to(units("degC"))
    #         Td2m_K = data_2m_pt.d2m.data * units("K")
    #         Td2m_C = Td2m_K.to(units("degC"))
    #         p2m_mb = psfc_mb - (0.2 * units("hPa"))
    #         lvl2m_m = groundlvl_m + (2.0 * units("m"))

    #         data_10m = xr.open_dataset(selected_file['file_path'], 
    #                  engine = engine,
    #                  filter_by_keys = {'typeOfLevel': 'heightAboveGround', 'level': 10},
    #                  errors='ignore')
    #         data_10m_pt = data_10m.sel(latitude=sounding_lat, longitude=sounding_lon, method='nearest')

    #         u10m_ms = data_10m_pt.u10.data * units("m/s")
    #         u10m_kt = u10m_ms.to(units("knot"))
    #         v10m_ms = data_10m_pt.v10.data * units("m/s")
    #         v10m_kt = v10m_ms.to(units("knot"))

    #         ua_df = pd.DataFrame(data=[p_mb.magnitude, T_C.magnitude, Td_C.magnitude, u_kt.magnitude, v_kt.magnitude, z_gpm.magnitude])
    #         sfc_df = pd.DataFrame(data=[p2m_mb.magnitude, T2m_C.magnitude, Td2m_C.magnitude, u10m_kt.magnitude, v10m_kt.magnitude, lvl2m_m.magnitude])
    #         sounding_df = pd.concat([sfc_df.T, ua_df.T],
    #                                 axis=0,
    #                                 ignore_index=True)

    #     #Remove NaN's for winds
    #     sounding_df[3].fillna("", inplace=True)
    #     sounding_df[4].fillna("", inplace=True)

    #     #Filter out levels below the surface
    #     sounding_df = sounding_df[sounding_df[0] <= psfc_mb.magnitude]
    #     sounding_df = sounding_df.round(decimals=2)

    #     if not sounding_name:
    #         raob_id = selected_file['file_name']
    #         file_name = f"RAOBsounding_pt{time_obj.strftime('%Y%m%d_%H%M%SZ')}_{self.selected_chunk['model_name']}{file_time_obj.strftime('%Y%m%d_%H%M%SZ')}.csv"
    #     else:
    #         raob_id = sounding_name
    #         file_name = f"{sounding_name}_RAOBsounding_pt{time_obj.strftime('%Y%m%d_%H%M%SZ')}_{self.selected_chunk['model_name']}{file_time_obj.strftime('%Y%m%d_%H%M%SZ')}.csv"

    #     raob_header = {0:['RAOB/CSV','DTG','LAT','LON','ELEV','MOISTURE','WIND','GPM','MISSING','RAOB/DATA','PRES'],
    #                    1:[raob_id,selected_file['time'],selected_point[0],selected_point[1],round(lvl2m_m.magnitude),'TD','kts','MSL',-999,'','TEMP'],
    #                    2:['','','N','W','m','','U/V','','','','TD'],
    #                    3:['','','','','','','','','','','UU'],
    #                    4:['','','','','','','','','','','VV'],
    #                    5:['','','','','','','','','','','GPM']}

    #     header_df = pd.DataFrame(data=raob_header)
    #     raob_df = pd.concat([header_df,sounding_df], axis=0, ignore_index=True)

    #     output_dir = os.path.join(self.working_dir, 'RAOB Soundings', self.selected_chunk['model_name'])
    #     if not os.path.exists(output_dir):
    #         os.makedirs(output_dir)

    #     output_path = os.path.join(output_dir, file_name)
    #     raob_df.to_csv(output_path, index=False, header=False)
